{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import os\nimport json\nimport time\nfrom datetime import datetime",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def get_data_into_raw(endpoint):\n    \"\"\"\n    Fetches data from the specified endpoint and saves it in it's original format in raw layer.\n    It also adds two metadata columns (i.e. extract_time & source) to the dataset. Every api call \n    will create a new file in the raw layer. The name of the file has a timestamp which we use to \n    determine which one is the latest dataset to be processed.\n    \n\n    Parameters:\n    - endpoint (str): The URL endpoint to fetch data from.\n\n    Returns:\n    - None\n    \"\"\"\n    current_time = datetime.now()\n\n    data = get_data(endpoint)\n    data['extract_time'] = datetime.now().timestamp()\n    data['source'] = 'exchange rates api'\n\n    with open(f'../../data/raw/raw_rates_{current_time}.json', 'w') as json_file:\n        json.dump(data, json_file)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def get_data_into_structured():\n    \"\"\"\n    Reads the latest raw JSON data file from the raw layer, transform it, enforce schema and write \n    it into structured layer. Here we also add a number of metadata columns (i.e. source_file, \n    extract_time & current) to the dataset. In this layer we perform a crude SCD with a column \n    named 'current' which holds a '1' for latest records and a '0' for historical records.\n\n    Parameters:\n    - None\n    \n    Returns:\n    - None\n    \"\"\"\n    # First we need to find the latest file in the raw zone\n    directory = '../../data/raw/'\n    files = os.listdir(directory)\n    \n    if not files:\n        return None\n        \n    latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(directory, f)))\n\n    # We then extract the rates from this file, clean and enforce schema and load it into a dataframe\n    with open(f'{directory}/{latest_file}', \"r\") as file:\n        json_data = json.load(file)\n    \n    new_rates_df = get_rates_dataframe(json_data)\n\n    extract_time = datetime.now().timestamp()\n    new_rates_df = new_rates_df.assign(source_file=lambda x: latest_file)\n    new_rates_df = new_rates_df.assign(extract_time=lambda x: extract_time)\n\n    # current = 1 for records in the latest batch\n    new_rates_df = new_rates_df.assign(current=lambda x: 1) \n\n    # If structured has previouosly been processed we set current = 0 for all records\n    structured_path = '../../data/structured/structured_rates.json'\n    if os.path.exists(structured_path):\n        with open(structured_path, 'r') as file:\n            data = json.load(file)\n\n        old_rates_df = pd.DataFrame(data)\n        old_rates_df['current'] = 0\n        \n        combined_df = pd.concat([old_rates_df, new_rates_df], ignore_index=True)\n\n        os.remove(structured_path)\n    else:\n        combined_df = new_rates_df\n\n    combined_df.to_json(structured_path, orient='records')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def get_data_into_curated():\n    \"\"\"\n    Reads the structured file from structured layer. We perform aggregation here.\n\n    Parameters:\n    - None\n    \n    Returns:\n    - None\n    \"\"\"\n    # Read latest rates from structured_rates.json\n    structured_path = '../../data/structured/structured_rates.json'\n    if not os.path.exists(structured_path):\n        return\n        \n    with open(structured_path, 'r') as file:\n        data = json.load(file)\n\n    structured_rates_df = pd.DataFrame(data)\n\n    latest_df = structured_rates_df[structured_rates_df['current'] == 1]\n\n    summary_path = '../../data/curated/summary_rates.json'\n    if os.path.exists(summary_path):\n        os.remove(summary_path)\n\n    statistics_df = latest_df.groupby(['currency'], as_index=False).agg(\n        mean_rate=('rate', 'mean'),\n        worst_rate=('rate', 'min'),\n        best_rate=('rate', 'max')\n    )\n\n    statistics_df.to_json(summary_path, orient='records')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def run_pipeline(endpoint):\n    \"\"\"\n    Orchestrates the end to end pipeline, brings data from source api into raw, structured & curated layers.\n\n    Parameters:\n    - endpoint (str): The URL endpoint to fetch data from.\n    \n    Returns:\n    - None\n    \"\"\"\n    get_data_into_raw(endpoint)\n    get_data_into_structured()\n    get_data_into_curated()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}