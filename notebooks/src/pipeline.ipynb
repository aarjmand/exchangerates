{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import os\nimport json\nimport time\nimport shutil\nfrom datetime import datetime",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def get_data_into_raw(endpoint):\n    \"\"\"\n    Fetches data from the specified endpoint and saves it in it's original format in raw layer.\n    It also adds two metadata columns (i.e. extract_time & source) to the dataset. Every api call \n    will create a new file in the raw layer. The name of the file has a timestamp which we use to \n    determine which one is the latest dataset to be processed.\n    \n    Parameters:\n    - endpoint (str): The URL endpoint to fetch data from.\n\n    Returns:\n    - None\n    \"\"\"\n    try:\n        current_time = datetime.now()\n    \n        data = get_data(endpoint)\n        data['extract_time'] = datetime.now().timestamp()\n        data['source'] = 'exchange rates api'\n    \n        with open(f'../../data/raw/raw_rates_{current_time}.json', 'w') as json_file:\n            json.dump(data, json_file)\n    except Exception as e:\n        raise e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def get_data_into_structured(latest_file):\n    \"\"\"\n    Reads the latest raw JSON data file from the raw layer, transform it, enforce schema and write \n    it into structured layer. Here we also add a number of metadata columns (i.e. source_file, \n    extract_time & current) to the dataset. In this layer we perform a crude SCD2 with a column \n    named 'current' which holds a '1' for latest records and a '0' for historical records.\n\n    Parameters:\n    - latest_file (str): Path to the latest file written to Raw layer\n    \n    Returns:\n    - None\n    \"\"\"\n    try:\n        # We then extract the rates from this file, clean and enforce schema and load it into a dataframe\n        with open(latest_file, \"r\") as file:\n            json_data = json.load(file)\n    \n        base_currency = json_data['base']\n        new_rates_df = get_rates_dataframe(json_data)\n    \n        extract_time = datetime.now().timestamp()\n        new_rates_df = new_rates_df.assign(base=lambda x: base_currency)\n        new_rates_df = new_rates_df.assign(source_file=lambda x: latest_file)\n        new_rates_df = new_rates_df.assign(extract_time=lambda x: extract_time)\n    \n        # current = 1 for records in the latest batch\n        new_rates_df = new_rates_df.assign(current=lambda x: 1) \n        \n        # If structured has previouosly been processed we set current = 0 for all records\n        structured_path = '../../data/structured/structured_rates.json'\n        if os.path.exists(structured_path):\n            with open(structured_path, 'r') as file:\n                data = json.load(file)\n    \n            old_rates_df = pd.DataFrame(data)\n            old_rates_df['current'] = 0\n            \n            combined_df = pd.concat([old_rates_df, new_rates_df], ignore_index=True)\n    \n            os.remove(structured_path)\n        else:\n            combined_df = new_rates_df\n    \n        combined_df.to_json(structured_path, orient='records')\n    except Exception as e:\n        raise e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def get_data_into_curated():\n    \"\"\"\n    Reads the structured file from structured layer. We perform aggregation here.\n\n    Parameters:\n    - None\n    \n    Returns:\n    - None\n    \"\"\"\n    try:\n        # Read latest rates from structured_rates.json\n        structured_path = '../../data/structured/structured_rates.json'\n        if not os.path.exists(structured_path):\n            return\n            \n        with open(structured_path, 'r') as file:\n            data = json.load(file)\n    \n        structured_rates_df = pd.DataFrame(data)\n    \n        latest_df = structured_rates_df[structured_rates_df['current'] == 1]\n    \n        summary_path = '../../data/curated/summary_rates.json'\n        if os.path.exists(summary_path):\n            os.remove(summary_path)\n    \n        statistics_df = latest_df.groupby(['currency', 'base'], as_index=False).agg(\n            mean_rate=('rate', 'mean'),\n            worst_rate=('rate', 'min'),\n            best_rate=('rate', 'max')\n        )\n        \n        statistics_df.to_json(summary_path, orient='records')\n    except Exception as e:\n        raise e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def run_pipeline(endpoint):\n    \"\"\"\n    Orchestrates the end to end pipeline, brings data from source api into raw, structured & curated layers.\n\n    Parameters:\n    - endpoint (str): The URL endpoint to fetch data from.\n    \n    Returns:\n    - None\n    \"\"\"\n    try:\n        # Call rates api and process data into Raw layer\n        get_data_into_raw(endpoint)\n        \n        # First we need to find the latest file in raw layer\n        directory = '../../data/raw/'\n        files = os.listdir(directory)\n        \n        if not files:\n            return None\n\n        latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(directory, f)))\n\n        # Get raw data, transform & save into Structured layer\n        get_data_into_structured(f'{directory}{latest_file}')\n    \n        # Aggregte Structured data and save into Curated layer\n        get_data_into_curated()\n    except Exception as e:\n        raise e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def rebuild_structured_and_curated():\n    \"\"\"\n    In the event of a catastrophe, this function will reconstruct the structured and curated layers using data from \n    the raw layer, provided that the raw layer remains unaffected by the event.\n\n    Parameters:\n    - None\n    \n    Returns:\n    - None\n    \"\"\"\n    try:\n        # Delete the files from Structured & Curated layers\n        for directory in ['../../data/structured/', '../../data/curated/']:\n            for filename in os.listdir(directory):\n                # Check if the file has a .json extension\n                if filename.endswith(\".json\"):\n                    filepath = os.path.join(directory, filename)\n                    os.remove(filepath)\n\n        # Get a list of all files in Raw layer\n        raw_directory = '../../data/raw/'\n        files = os.listdir(raw_directory)\n    \n        # Sort files in ascending order\n        files.sort(reverse=False)\n    \n        for f in files:\n            get_data_into_structured(f'{raw_directory}{f}')\n\n        # Get data into Curated layer from newly reconstructed Structured layer\n        get_data_into_curated()\n    except Exception as e:\n        raise e  ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
